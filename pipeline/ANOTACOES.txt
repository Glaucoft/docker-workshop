CURSO:  https://courses.datatalks.club/
        https://github.com/DataTalksClub/data-engineering-zoomcamp
        https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
        https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf
SOURCE: https://github.com/DataTalksClub/nyc-tlc-data/releases


docker run -it --rm \
  -e POSTGRES_USER="root" \
  -e POSTGRES_PASSWORD="root" \
  -e POSTGRES_DB="ny_taxi" \
  -v ny_taxi_postgres_data:/var/lib/postgresql \
  -p 5432:5432 \
  --network=pg-network \
  --name pgdatabase \
  postgres:18

docker run -it \
  -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" \
  -e PGADMIN_DEFAULT_PASSWORD="root" \
  -v pgadmin_data:/var/lib/pgadmin \
  -p 8085:80 \
  --network=pg-network \
  --name pgadmin \
  dpage/pgadmin4

# Script de Ingest
docker run -it --rm \
  --network=pipeline_pg-network \
  taxi_ingest:v001 \
  --pg-user=root \
  --pg-pass=root \
  --pg-host=pgdatabase \
  --pg-port=5432 \
  --pg-db=ny_taxi \
  --target-table=yellow_taxi_trips_2021_1 \
  --year=2021 \
  --month=1 \
  --chunksize=100000

####################################################
COMANDOS: Executar a partir do diretório: /workspaces/docker-workshop/pipeline

docker network create pg-network
docker volume rm ny_taxi_postgres_data pgadmin_data
docker rm pgadmin

sudo mv <file> <dir>
--------------------------------------------------- < INÍCIO >
-- Ante de iniciar verifique se o docker e volume estão rodando/montados;
docker ps -a
docker volume ls

-- Se estiver rodando/montados, devem ser parados/desmontados antes de iniciar o pipeline/ingest:
docker-compose down -v
docker volume rm <vol_name>

-- No docker-compose sobem os serviços, por exemplo: Banco e pgAdmin
O INGEST não é um serviço, mas sim um JOB, desta forma é executado de forma apartada

1) Subir os serviços via docker-compose;
       docker-compose up -d
       docker compose up -d

2) Fazer o ingest;

-- Console pgAdmin
127.0.0.1:8085
Servers > Register > Server...
General-Name: pg
Connections-host=pgdatabase/Username_and_Password=root
SAVE
--------------------------------------------------- < FIM >

OBSERVAÇÃO: Após alterações no arquivo de ingest, sempre dar o BUILD   <<<<<<<<<<---     A T E N Ç Ã O
docker build -t taxi_ingest:v001 .
docker build -t taxi_ingest:v002 .


Caminho                   Existe quando?
/app	                    Só se estiver no Dockerfile
/data	                    Só se você montar com -v
/workspaces/...	          ❌ nunca existe no container


# Script de Ingest HOMEWORK (Executar a partir do diretório: /workspaces/docker-workshop/pipeline)
docker run -it --rm \
  --network=pipeline_pg-network \
  -v $(pwd)/data:/data \
  taxi_ingest:v002 \
  --pg-user=root \
  --pg-pass=root \
  --pg-host=pgdatabase \
  --pg-port=5432 \
  --pg-db=ny_taxi \
  --target-table=taxi_zone_lookup \
  --chunksize=100000 \
  --input-file=/data/taxi_zone_lookup.csv

BACKUP:
  --target-table=green_taxi_trips_2025_11 \
  --input-file=/data/green_tripdata_2025-11.parquet
  --year=2025 \
  --month=11 \



--- Consulta SQL no próprio Terminal:
docker exec -it pgdatabase psql -U root -d ny_taxi
SELECT COUNT(*) FROM yellow_taxi_trips_2025_11;
\q


--- Teste de leitura do arquivo:
$ docker run --rm \
  --entrypoint python \
  -v $(pwd)/data:/data \
  taxi_ingest:v002 \
  -c "import pyarrow.parquet as pq; t=pq.read_table('/data/yellow_tripdata_2025-11.parquet'); print(t.num_rows)"

-- Limpeza/Liberação de espaço no Codespace:
df -h
docker system prune -af --volumes
pip cache purge
npm cache clean --force
sudo apt-get clean
sudo apt-get autoremove -y
sudo rm -rf /var/lib/apt/lists/*


-- Verifica se o container está visualizando a wallet
docker exec -it pipeline-kestra-1 ls -l /kestra/wallet


docker compose build --no-cache
docker restart kestra

  ############################################## H O M E W O R K ############################################
[01-docker-terraform]

-- Dimensional
select *
from public.taxi_zone_lookup tz
;

-- Fato
select *
from public.green_taxi_trips_2025_11 tt
;

-- Question 1. What's the version of pip in the python:3.13 image?
docker run --rm python:3.13 pip --version

-- Question 2. Given the docker-compose.yaml, what is the hostname and port that pgadmin should use to connect to the postgres database?
db:5432

-- Question 3. For the trips in November 2025, how many trips had a trip_distance of less than or equal to 1 mile? (1 point)
select min(tt.lpep_pickup_datetime), max(tt.lpep_pickup_datetime),
	   min(tt.lpep_dropoff_datetime), max(tt.lpep_dropoff_datetime)
--  select *
from public.green_taxi_trips_2025_11 tt  -- 46.912
where 1=1
--and tt.lpep_pickup_datetime >= to_date('01/11/2025 00:00:00','dd/mm/yyyy hh24:mi:ss')
--and tt.lpep_pickup_datetime <= to_date('30/11/2025 23:59:59','dd/mm/yyyy hh24:mi:ss')
and tt.trip_distance <= 1
;

-- Question 4. Which was the pick up day with the longest trip distance? Only consider trips with trip_distance less than 100 miles.
select to_char(tt.lpep_pickup_datetime, 'dd/mm/yyyy'), max(tt.trip_distance)
from public.green_taxi_trips_2025_11 tt  -- 46.912
where 1=1
and tt.lpep_pickup_datetime >= to_date('14/11/2025 00:00:00','dd/mm/yyyy hh24:mi:ss')
and tt.lpep_pickup_datetime <= to_date('26/11/2025 23:59:59','dd/mm/yyyy hh24:mi:ss')
and tt.trip_distance <= 100
group by to_char(tt.lpep_pickup_datetime, 'dd/mm/yyyy')
order by to_char(tt.lpep_pickup_datetime, 'dd/mm/yyyy')
;


-- Question 5. Which was the pickup zone with the largest total_amount (sum of all trips) on November 18th, 2025? 
select  to_char(tt.lpep_pickup_datetime, 'dd/mm/yyyy'), z."Zone", sum(tt.total_amount)
from public.green_taxi_trips_2025_11 tt
join public.taxi_zone_lookup z
on 1=1
and tt."PULocationID" = z."LocationID"
and tt.lpep_pickup_datetime >= to_date('18/11/2025 00:00:00','dd/mm/yyyy hh24:mi:ss')
and tt.lpep_pickup_datetime <= to_date('19/11/2025 23:59:59','dd/mm/yyyy hh24:mi:ss')
and z."Zone" in ('East Harlem North', 'East Harlem South', 'Morningside Heights', 'Forest Hills')
group by to_char(tt.lpep_pickup_datetime, 'dd/mm/yyyy'), z."Zone"
order by sum(tt.total_amount)
;

-- Question 6. For the passengers picked up in the zone named "East Harlem North" in November 2025, which was the drop off zone that had the largest tip?
select z."Zone", z2."Zone", max(tt.tip_amount)
from public.green_taxi_trips_2025_11 tt
join public.taxi_zone_lookup z
on 1=1
and tt."PULocationID" = z."LocationID"
and z."Zone" in ('East Harlem North')
left join public.taxi_zone_lookup z2
on tt."DOLocationID" = z2."LocationID"
and z2."Zone" in ('JFK Airport', 'Yorkville West', 'East Harlem North', 'LaGuardia Airport')
and z2."Zone" is not null
group by z."Zone", z2."Zone"
order by max(tt.tip_amount)
;

-- Question 7. Which of the following sequences describes the Terraform workflow for: 1) Downloading plugins and setting up backend, 2) Generating and executing changes, 3) Removing all resources?
terraform init, terraform apply -auto-approve, terraform destroy

---------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------
[02-workflow-orchestration]

-- Question 1. Within the execution for Yellow Taxi data for the year 2020 and month 12: what is the uncompressed file size (i.e. the output file yellow_tripdata_2020-12.csv of the extract task)?
128 MB (134.483.968 bytes)

-- Question 2. What is the rendered value of the variable file when the inputs taxi is set to green, year is set to 2020, and month is set to 04 during execution? (1 point)
green_tripdata_2020-04.csv

-- Question 3. How many rows are there for the Yellow Taxi data for all CSV files in the year 2020? (1 point)
select count(*) 
from admin.EXT_YELLOW_TRIPDATA;
24.648.499

-- Question 4. How many rows are there for the Green Taxi data for all CSV files in the year 2020?
select count(*)
from admin.EXT_GREEN_TRIPDATA gt;
1.734.051

-- Question 5. How many rows are there for the Yellow Taxi data for the March 2021 CSV file? (1 point)
1.925.152

-- Question 6. How would you configure the timezone to New York in a Schedule trigger? (1 point)
Add a timezone property set to America/New_York in the Schedule trigger configuration
---------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------

[03-data-warehouse]

-- Question 1. What is count of records for the 2024 Yellow Taxi Data? (1 point)
EXPLAIN PLAN FOR
select count(*)
from admin.EXT_YELLOW_TRIPDATA_PQ
;
20.332.093

-- Question 2. What is the estimated amount of data that will be read when this query is executed on the External Table and the Table? (1 point)
0 MB for the External Table and 155.12 MB for the Materialized Table

-- External table
SELECT
  sql_id,
  elapsed_time,
  buffer_gets,
  disk_reads
FROM v$sql
WHERE sql_text LIKE '%EXT_YELLOW_TRIPDATA_PQ%';

-- Materialized table
SELECT
  segment_name,
  bytes / 1024 / 1024 AS size_mb
FROM dba_segments
WHERE segment_name = 'EXT_YELLOW_TRIPDATA_PQ_MT';


-- Question 3. Why are the estimated number of Bytes different? (1 point)
BigQuery is a columnar database, and it only scans the specific columns requested in the query. 
Querying two columns (PULocationID, DOLocationID) requires reading more data than querying one column (PULocationID), leading to a higher estimated number of bytes processed.

-- Question 4. How many records have a fare_amount of 0? (1 point)
select count(*)
from admin.EXT_YELLOW_TRIPDATA_PQ yt
where yt.fare_amount = 0;
8.333

-- Question 5. What is the best strategy to make an optimized table in Big Query if your query will always filter based on tpep_dropoff_datetime and order the results by VendorID (Create a new table with this strategy) (1 point)
Partition by tpep_dropoff_datetime and Cluster on VendorID

-- Cria a tabela particionada
CREATE TABLE YELLOW_TRIPDATA_OPT
PARTITION BY RANGE (tpep_dropoff_datetime)
INTERVAL (NUMTODSINTERVAL(1, 'DAY'))
(
  PARTITION p_start VALUES LESS THAN (DATE '2024-01-01')
)
AS
SELECT *
FROM EXT_YELLOW_TRIPDATA_PQ;

-- Valida o particionamento
SELECT
  partition_name,
  high_value
FROM user_tab_partitions
WHERE table_name = 'YELLOW_TRIPDATA_OPT'
ORDER BY partition_position;

-- Otimiza as queries
ALTER TABLE YELLOW_TRIPDATA_OPT INMEMORY;

-- Cria o índice para o order by frequente
CREATE INDEX IDX_YTD_VENDOR
ON YELLOW_TRIPDATA_OPT (VendorID);


-- Question 6. Write a query to retrieve the distinct VendorIDs between tpep_dropoff_datetime 2024-03-01 and 2024-03-15 (inclusive). Use the materialized table you created earlier in your from clause and note the estimated bytes. Now change the table in the from clause to the partitioned table you created for question 5 and note the estimated bytes processed. What are these values? (1 point)
310.24 MB for non-partitioned table and 26.84 MB for the partitioned table

-- Criação da tabela materializada
create table EXT_YELLOW_TRIPDATA_PQ_mt as 
select *
from admin.EXT_YELLOW_TRIPDATA_PQ
;

select count(distinct(mt.vendorid))
from admin.EXT_YELLOW_TRIPDATA_PQ_mt mt
where 1=1
and mt.tpep_dropoff_datetime >= to_date('01/03/2024 00:00:00','dd/mm/yyyy hh24:MI:ss')
and mt.tpep_dropoff_datetime <= to_date('15/03/2024 23:59:59','dd/mm/yyyy hh24:MI:ss')
;

SELECT
  sql_id,
  elapsed_time,
  buffer_gets,
  disk_reads
--  select *
FROM v$sql
WHERE sql_text LIKE '%count(distinct(mt.vendorid))%';
Disk_reads = 49117 x 8Kb = 392Mb

select count(distinct(opt.vendorid))
from admin.YELLOW_TRIPDATA_OPT opt
where 1=1
and opt.tpep_dropoff_datetime >= to_date('01/03/2024 00:00:00','dd/mm/yyyy hh24:MI:ss')
and opt.tpep_dropoff_datetime <= to_date('15/03/2024 23:59:59','dd/mm/yyyy hh24:MI:ss')
;

SELECT
  sql_id,
  elapsed_time,
  buffer_gets,
  disk_reads
--  select *
FROM v$sql
WHERE sql_text LIKE '%count(distinct(opt.vendorid))%';
Disk_reads = 4246 x 8Kb = 33Mb

-- Question 7. Where is the data stored in the External Table you created? (1 point)
GCP Bucket

-- Question 8. It is best practice in Big Query to always cluster your data: (1 point)
False

-- Question 9. Write a `SELECT count(*)` query FROM the materialized table you created. How many bytes does it estimate will be read? Why? (not graded)
select count(*)
from admin.EXT_YELLOW_TRIPDATA_PQ_mt mt;

select *
FROM v$sql
WHERE sql_text LIKE '%count(*)%admin.EXT_YELLOW_TRIPDATA_PQ_mt mt%';
Disk_reads = 4 x 8Kb = 32Kb

The estimated bytes read is close to zero.
This happens because the query is a simple COUNT(*) over a materialized table, and the database engine can retrieve the row count from metadata and statistics without scanning the actual data blocks.
---------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------